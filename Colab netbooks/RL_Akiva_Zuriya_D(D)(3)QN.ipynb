{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_Akiva_Zuriya_D(D)(3)QN",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "Alyw3SfhMcqc",
        "SkIFgIt5Razq",
        "8xYsQXveSf0D",
        "KLQw5FCF3bzp",
        "GxSvluYoc_ia",
        "JersW2jadkyy",
        "lO_e1zF4dtbB",
        "YT3FX-6Ad_ry",
        "zyCkyF3Ghib2",
        "hvMyckEEvhI-"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1><center><b>REINFORCEMENT LEARNING</b></center></h1>\n",
        "<h1><center><b>Final Project - Part 1.1</b></center></h1>"
      ],
      "metadata": {
        "id": "baOu2XRlv-Eo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><center>Lunar Lander v2</center></h3>\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "<h3><center>DQN, DDQN, and D3QN</center></h3>"
      ],
      "metadata": {
        "id": "iFFFdGhILbfm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Names:  \n",
        "<font color='red'>\n",
        "Zuriya Ansbacher ID. 208532515  \n",
        "Akiva Bruno Melka ID. 332629393  \n",
        "</font>"
      ],
      "metadata": {
        "id": "g2Im8KBnwHn-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/zuriyaAnsbacher/Reinforcement_Project"
      ],
      "metadata": {
        "id": "krn7-0FXQ2ak"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Install Environment**"
      ],
      "metadata": {
        "id": "Alyw3SfhMcqc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ti31OH3g6gz"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get update\n",
        "!pip install gym\n",
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install piglet\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "!pip install gym[box2d]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This code creates a virtual display to draw game images on. \n",
        "# If you are running locally, just ignore it\n",
        "import os\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
        "    !bash ../xvfb start\n",
        "    %env DISPLAY=:1"
      ],
      "metadata": {
        "id": "e5VYiw4XhA_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Import Libraries**"
      ],
      "metadata": {
        "id": "SkIFgIt5Razq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) # error only\n",
        "from gym.wrappers.monitoring import video_recorder\n",
        "\n",
        "import os\n",
        "import copy\n",
        "import random\n",
        "import argparse\n",
        "import numpy as np\n",
        "from statistics import mean\n",
        "from collections import deque\n",
        "from dataclasses import dataclass\n",
        "from itertools import product\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import tensorflow as tf\n",
        "import torch.optim as optim\n",
        "from torch.optim import Adam\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay"
      ],
      "metadata": {
        "id": "l14BPIxPhDbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Utility functions**"
      ],
      "metadata": {
        "id": "8xYsQXveSf0D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.2 Miscellaneous**"
      ],
      "metadata": {
        "id": "KLQw5FCF3bzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the seed\n",
        "def set_seed(env, seed=0):\n",
        "    np.random.seed(seed)\n",
        "    env.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# Discretization of the action space\n",
        "def quantize_space(actions_range, bins):\n",
        "    discrete_actions = product(*[np.linspace(start, end, num=num_of_splits) for\n",
        "                                 (start, end), num_of_splits in zip(actions_range, bins)])\n",
        "    return list(discrete_actions)\n",
        "\n",
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video(file):\n",
        "  mp4list = glob.glob(file)\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "\n",
        "    \n",
        "def create_plot(scores, model_name, title, avg_lag):\n",
        "    \n",
        "    avg = []\n",
        "    for i in range(len(scores)):\n",
        "        j = 0 if i <= avg_lag - 1 else i - (avg_lag - 1)\n",
        "        avg.append(mean(scores[j:i+1]))\n",
        "\n",
        "    fig, axis = plt.subplots()\n",
        "    axis.clear()\n",
        "    axis.plot(scores, 'c', label='Score', alpha=0.7)\n",
        "    axis.plot(avg, 'orange', label='Average score')\n",
        "    axis.axhline(200, c='gray', label='Goal', alpha=0.7)\n",
        "    axis.set_xlabel('Episodes')\n",
        "    axis.set_ylabel('Scores')\n",
        "    axis.legend(loc='lower right')\n",
        "    plt.title(title + f'Average score: {mean(scores):.02f}')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "CYIJQv3OhtiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.2 Memory replay class**"
      ],
      "metadata": {
        "id": "GxSvluYoc_ia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@ dataclass\n",
        "class Sample:\n",
        "    state: np.ndarray\n",
        "    action: int or np.ndarray\n",
        "    reward: float\n",
        "    next_state: np.ndarray\n",
        "    done: bool\n",
        "\n",
        "\n",
        "class SamplesMemory:\n",
        "    def __init__(self, max_size, device):\n",
        "        self.max_size = max_size\n",
        "        self.device = device\n",
        "        self.memory_buffer = deque(maxlen=max_size)  # maxlen ensure that samples num won't exceed\n",
        "\n",
        "    def add_sample(self, state, action, reward, next_state, done):\n",
        "        sample = Sample(state, action, reward, next_state, done)\n",
        "        self.memory_buffer.append(sample)\n",
        "\n",
        "    def get_batch(self, batch_size, continuous_action=False):\n",
        "        batch = random.sample(self.memory_buffer, batch_size)\n",
        "        f = lambda x, my_type: torch.tensor(np.vstack(x), device=self.device, dtype=my_type)\n",
        "\n",
        "        state_batch = f([sample.state for sample in batch], torch.float)\n",
        "        action_batch = f([sample.action for sample in batch], torch.float) if continuous_action \\\n",
        "            else f([sample.action for sample in batch], torch.long)\n",
        "        reward_batch = f([sample.reward for sample in batch], torch.float)\n",
        "        next_state_batch = f([sample.next_state for sample in batch], torch.float)\n",
        "        done_batch = f([sample.done for sample in batch], torch.float)\n",
        "\n",
        "        return state_batch, action_batch, reward_batch, next_state_batch, done_batch\n",
        "\n"
      ],
      "metadata": {
        "id": "F6TZNnebiT7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Agents**"
      ],
      "metadata": {
        "id": "JersW2jadkyy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.1 Networks architecture**"
      ],
      "metadata": {
        "id": "lO_e1zF4dtbB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Models\"\"\"\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidd1_size, hidd2_size):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Sequential(nn.Linear(input_size, hidd1_size), nn.ReLU())\n",
        "        self.fc2 = nn.Sequential(nn.Linear(hidd1_size, hidd2_size), nn.ReLU())\n",
        "        self.fc3 = nn.Linear(hidd2_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DuelingDQN(nn.Module):\n",
        "    def __init__(self, input_size, output_size, linear_hid_size, adv_hid_size, val_hid_size):\n",
        "        super(DuelingDQN, self).__init__()\n",
        "\n",
        "        # common linear layer\n",
        "        self.linear1 = nn.Linear(input_size, linear_hid_size)\n",
        "\n",
        "        # 2 linear layers for advantage calculation\n",
        "        self.linear_adv_1 = nn.Linear(linear_hid_size, adv_hid_size)\n",
        "        self.linear_adv_2 = nn.Linear(adv_hid_size, output_size)\n",
        "\n",
        "        # 2 linear layers for value calculation\n",
        "        self.linear_val_1 = nn.Linear(linear_hid_size, val_hid_size)\n",
        "        self.linear_val_2 = nn.Linear(val_hid_size, 1)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.linear1(state))\n",
        "        adv = self.linear_adv_2(F.relu(self.linear_adv_1(x)))\n",
        "        val = self.linear_val_2(F.relu(self.linear_val_1(x)))\n",
        "\n",
        "        return val + (adv - adv.mean())\n"
      ],
      "metadata": {
        "id": "VpkUcIQViYqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.2 Super Agent class**"
      ],
      "metadata": {
        "id": "YT3FX-6Ad_ry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Agent \"\"\"\n",
        "class Agent:\n",
        "    def __init__(self, action_space, output_size, batch_size, gamma, memory_size,\n",
        "                 max_eps, min_eps, eps_decay, target_update, device):\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.batch_size = batch_size\n",
        "        self.gamma = gamma\n",
        "        self.device = device\n",
        "\n",
        "        # epsilon parameters\n",
        "        self.min_eps = min_eps\n",
        "        self.eps_decay = eps_decay\n",
        "        self.cur_eps = max_eps\n",
        "\n",
        "        # updated parameters\n",
        "        self.target_update = target_update\n",
        "        self.update = 0\n",
        "        self.full_target = False\n",
        "\n",
        "        # will be defined in children classes\n",
        "        self.policy_net = None\n",
        "        self.target_net = None\n",
        "        self.optimizer = None\n",
        "        self.model_name = None\n",
        "        self.memory = SamplesMemory(memory_size, device)\n",
        "\n",
        "        self.idx2action = {i: action for i, action in enumerate(action_space)}\n",
        "        self.action2idx = {action: i for i, action in enumerate(action_space)}\n",
        "\n",
        "    def get_action(self, state, just_greedy=False):\n",
        "        # epsilon greedy\n",
        "        if not just_greedy and np.random.random() < self.cur_eps:  # exploration\n",
        "            return self.idx2action[np.random.choice(self.output_size)]\n",
        "        else:  # exploitation\n",
        "            with torch.no_grad():\n",
        "                state = torch.tensor(state, dtype=torch.float, device=self.device)\n",
        "                return self.idx2action[self.policy_net(state).argmax().item()]\n",
        "\n",
        "    def decrement_epsilon(self):\n",
        "        return max(self.cur_eps * self.eps_decay, self.min_eps)\n",
        "\n",
        "    def learn(self):\n",
        "        states, actions, rewards, next_states, dones = self.memory.get_batch(self.batch_size)\n",
        "        q_val = self.policy_net(states).gather(1, actions)\n",
        "\n",
        "        if self.full_target:  # ddqn, d3qn\n",
        "            with torch.no_grad():\n",
        "                next_q_values_argmax = self.policy_net(next_states).argmax(1)\n",
        "            next_q_val = self.target_net(next_states).gather(1, next_q_values_argmax.unsqueeze(1)).detach()\n",
        "        else:  # dqn\n",
        "            next_q_val = self.target_net(next_states).max(1, keepdim=True)[0].detach()\n",
        "\n",
        "        # formula\n",
        "        target = (rewards + self.gamma * next_q_val * (1 - dones)).to(self.device)\n",
        "\n",
        "        # loss and optimizer\n",
        "        self.optimizer.zero_grad()\n",
        "        loss = F.smooth_l1_loss(q_val, target)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.update += 1\n",
        "\n",
        "        # every target_update num, load weights from policy_net to target_net\n",
        "        if self.update % self.target_update == 0:\n",
        "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def set_eval(self):\n",
        "        self.policy_net.eval()\n",
        "\n",
        "    def save(self, file_path):\n",
        "        torch.save(self.policy_net.state_dict(), file_path)\n",
        "\n",
        "    def load(self, file_path):\n",
        "        self.policy_net.load_state_dict(torch.load(file_path, map_location=self.device))\n",
        "\n",
        "    def test(self, env, model_name, record, record_freq, episode_num=100, steps_num=1000):\n",
        "\n",
        "      print(\"Start evaluation...\")\n",
        "\n",
        "      self.set_eval()\n",
        "      scores_list = []\n",
        "      success = False\n",
        "\n",
        "      for episode in range(1, episode_num + 1):\n",
        "          state = env.reset()\n",
        "          score = 0\n",
        "\n",
        "          for step in range(1, steps_num + 1):\n",
        "\n",
        "              action = self.get_action(state, just_greedy=True)\n",
        "              next_state, reward, done, _ = env.step(action)\n",
        "              score += reward\n",
        "              state = next_state\n",
        "              if done: \n",
        "                  break\n",
        "\n",
        "          scores_list.append(score)\n",
        "          print(f'Episode: {episode}, Test score: {score:.02f}')\n",
        "\n",
        "      if mean(scores_list) >= 200:\n",
        "          success = True\n",
        "          create_plot(scores_list, self.model_name, f'{self.model_name} Test.', episode_num)\n",
        "\n",
        "      else:\n",
        "          print(f'The evaluation has not achieved the goal yet. The model will be training more..')\n",
        "\n",
        "      return success\n",
        "\n",
        "    def train(self, env, episodes_num, steps_num, learn_freq, \n",
        "              record, record_freq):\n",
        "        \n",
        "        scores_list = []\n",
        "        count_test = 1\n",
        "\n",
        "        vid_path = f\"./video.mp4\"\n",
        "        video = video_recorder.VideoRecorder(env, path=vid_path) \n",
        "        video.enabled = False\n",
        "\n",
        "        self.policy_net.train()\n",
        "        for episode in range(1, episodes_num + 1):\n",
        "            print(f'Episode: {episode}', end='')\n",
        "            state = env.reset()\n",
        "            score = 0 \n",
        "\n",
        "            for step in range(1, steps_num + 1):  \n",
        "\n",
        "                if (episode - 1) % record_freq == 0 and record:\n",
        "                    env.render()\n",
        "                    video.enabled = True\n",
        "                    video.capture_frame()      \n",
        "\n",
        "                action = self.get_action(state)\n",
        "                next_state, reward, done, _ = env.step(action)\n",
        "                action = self.action2idx[action]\n",
        "\n",
        "                self.memory.add_sample(state, action, reward, next_state, done)\n",
        "                if len(self.memory.memory_buffer) > self.batch_size and step % learn_freq == 0:\n",
        "                    self.learn()\n",
        "\n",
        "                score += reward\n",
        "                state = next_state\n",
        "\n",
        "                if done:\n",
        "                    self.cur_eps = self.decrement_epsilon()\n",
        "                    break\n",
        "\n",
        "            print(f'| Score: {score}')\n",
        "            scores_list.append(score)\n",
        "\n",
        "            if len(scores_list) >= 50 and mean(scores_list[-10:]) >= 200 and episode >= count_test + 10:\n",
        "              count_test = episode\n",
        "              success = self.test(env, self.model_name, record, record_freq)\n",
        "              if success:\n",
        "                print(f'Mission accomplished in episode {episode}!')\n",
        "                create_plot(scores_list, self.model_name, f'{self.model_name} Train.', 10)\n",
        "                break\n",
        "   \n",
        "        video.close()\n",
        "        show_video(vid_path)\n",
        "        env.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "uUCJJ2gdiaA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.3 Models sub-classes**"
      ],
      "metadata": {
        "id": "zyCkyF3Ghib2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNAgent(Agent):\n",
        "    def __init__(self, input_size, output_size, action_space, batch_size, lr, gamma, eps_decay,\n",
        "                 target_update, hidden_layers_size, memory_size, max_eps, min_eps, device):\n",
        "        super(DQNAgent, self).__init__(action_space, output_size, batch_size, gamma, memory_size, max_eps,\n",
        "                                       min_eps, eps_decay, target_update, device)\n",
        "\n",
        "        self.model_name = 'DQN'\n",
        "        self.hidd1 = hidden_layers_size[0]\n",
        "        self.hidd2 = hidden_layers_size[1]\n",
        "        self.policy_net = DQN(input_size, self.output_size, self.hidd1, self.hidd2).to(device)\n",
        "        self.target_net = DQN(input_size, self.output_size, self.hidd1, self.hidd2).to(device)  # copy.deepcopy?\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.target_net.eval()\n",
        "        self.full_target = False  # unnecessary\n",
        "\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "class DoubleDQNAgent(DQNAgent):\n",
        "    def __init__(self, input_size, output_size, action_space, batch_size, lr, gamma, eps_decay,\n",
        "                 target_update, hidden_layers_size, memory_size, max_eps, min_eps, device):\n",
        "        super(DoubleDQNAgent, self).__init__(input_size, output_size, action_space, batch_size, lr, gamma, eps_decay,\n",
        "                                             target_update, hidden_layers_size, memory_size, max_eps, min_eps, device)\n",
        "\n",
        "        self.model_name = 'DoubleDQN'\n",
        "        self.full_target = True\n",
        "\n",
        "\n",
        "class DuelingDDQNAgent(Agent):\n",
        "    def __init__(self, input_size, output_size, action_space, batch_size, lr, gamma, eps_decay,\n",
        "                 target_update, hidden_layers_size, memory_size, max_eps, min_eps, device):\n",
        "        super(DuelingDDQNAgent, self).__init__(action_space, output_size, batch_size, gamma, memory_size,\n",
        "                                               max_eps, min_eps, eps_decay, target_update, device)\n",
        "\n",
        "        self.model_name = 'DuelingDDQN'\n",
        "        self.hid_size_linear = hidden_layers_size[0]\n",
        "        self.hid_size_adv = hidden_layers_size[1]\n",
        "        self.hid_size_val = hidden_layers_size[2]\n",
        "        self.policy_net = DuelingDQN(input_size, self.output_size, self.hid_size_linear, self.hid_size_adv,\n",
        "                                     self.hid_size_val).to(device)\n",
        "        self.target_net = DuelingDQN(input_size, self.output_size, self.hid_size_linear, self.hid_size_adv,\n",
        "                                     self.hid_size_val).to(device)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.target_net.eval()\n",
        "        self.full_target = True\n",
        "\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "xBdM-DMrhtUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.4 Models parameters optimized with NNI**"
      ],
      "metadata": {
        "id": "hvMyckEEvhI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"DQN\"\"\"\n",
        "best_params_dqn = {\"batch_size\": 256,\n",
        "    \"gamma\": 0.9791633725985145,\n",
        "    \"lr\": 0.0015830847211986054,\n",
        "    \"target_update\": 400,\n",
        "    \"learn_freq\": 3,\n",
        "    \"eps_decay\": 0.9590128678975974,\n",
        "    \"max_steps\": 900,\n",
        "    \"hidden_layers_size\": [256, 64]}\n",
        "\n",
        "\"\"\"DDQN\"\"\"\n",
        "best_params_ddqn = {\"batch_size\": 128,\n",
        "    \"gamma\": 0.9856368308835148,\n",
        "    \"lr\": 0.002066460106748642,\n",
        "    \"target_update\": 100,\n",
        "    \"learn_freq\": 8,\n",
        "    \"eps_decay\": 0.9826503107232751,\n",
        "    \"max_steps\": 500,\n",
        "    \"hidden_layers_size\": [64, 64]}\n",
        "\n",
        "\"\"\"D3QN\"\"\"\n",
        "best_params_d3qn = {\"batch_size\": 64,\n",
        "    \"gamma\": 0.9828312425604563,\n",
        "    \"lr\": 0.001920259375173547,\n",
        "    \"target_update\": 100,\n",
        "    \"learn_freq\": 4,\n",
        "    \"eps_decay\": 0.9519693635850672,\n",
        "    \"max_steps\": 1000,\n",
        "    \"hidden_layers_size\": [256, 32, 128]}"
      ],
      "metadata": {
        "id": "zANQJp6R1C1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Main**"
      ],
      "metadata": {
        "id": "w54WHsZSgUZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "  parser = argparse.ArgumentParser()\n",
        "\n",
        "  # args that change each run\n",
        "  parser.add_argument('--model', choices=['dqn', 'ddqn', 'd3qn'], default='d3qn')\n",
        "  parser.add_argument('--use_nni_params', default=True, help='if true, get params from json file')\n",
        "  parser.add_argument('--set_num', type=str, default='2')\n",
        "\n",
        "  # args that usually stay fixed\n",
        "  parser.add_argument('--memory_size', type=int, default=100000)\n",
        "  parser.add_argument('--episodes', type=int, default=800, help='number of episodes in train')\n",
        "  parser.add_argument('--cuda_device', type=int, default=0)\n",
        "  parser.add_argument('--max_eps', type=float, default=1.0)\n",
        "  parser.add_argument('--min_eps', type=float, default=0.01)\n",
        "  parser.add_argument('--record', type=bool, default=True)\n",
        "  parser.add_argument('--record_freq', type=int, default=10)\n",
        "\n",
        "  args = parser.parse_args(args=[])\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  print(device)\n",
        "\n",
        "  if args.model == 'dqn':\n",
        "      best_params = best_params_dqn\n",
        "  elif args.model == 'ddqn':\n",
        "      best_params = best_params_ddqn\n",
        "  elif args.model == 'd3qn':\n",
        "      best_params = best_params_d3qn\n",
        "\n",
        "  env = gym.make('LunarLanderContinuous-v2')\n",
        "  set_seed(env)\n",
        "  env.reset()\n",
        "\n",
        "  state_size = env.observation_space.shape[0] \n",
        "  action_space = quantize_space(actions_range=[(-1, 1), (-1, 1)], bins=[5, 5])\n",
        "\n",
        "  agent_params = [state_size, len(action_space), action_space,\n",
        "                    best_params['batch_size'], best_params['lr'], best_params['gamma'],\n",
        "                    best_params['eps_decay'], best_params['target_update'], best_params['hidden_layers_size'],\n",
        "                    args.memory_size, args.max_eps, args.min_eps, device]\n",
        "\n",
        "  if args.model == 'dqn':\n",
        "      agent = DQNAgent(*agent_params)\n",
        "  elif args.model == 'ddqn':\n",
        "      agent = DoubleDQNAgent(*agent_params)\n",
        "  elif args.model == 'd3qn':\n",
        "      agent = DuelingDDQNAgent(*agent_params)\n",
        "\n",
        "  train_params = [env, args.episodes, best_params['max_steps'], best_params['learn_freq'], args.record, args.record_freq]\n",
        "  agent.train(*train_params)\n",
        "  print(\"program is over\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()\n",
        "\n"
      ],
      "metadata": {
        "id": "c0it5Gujibhs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}