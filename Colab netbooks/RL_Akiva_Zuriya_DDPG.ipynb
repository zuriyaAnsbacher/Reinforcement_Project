{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_Akiva_Zuriya_DDPG",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "Xgd1cHlVtoEi",
        "A1xHTdRPtreo",
        "UP0pkOoJts10",
        "cCMKzrkft7Cw",
        "q9vPT060t-Q5"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1><center><b>REINFORCEMENT LEARNING</b></center></h1>\n",
        "<h1><center><b>Final Project - Part 1.2</b></center></h1>\n",
        "\n"
      ],
      "metadata": {
        "id": "KyFQsNjAtVIT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><center>Lunar Lander v2</center></h3>\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "<h3><center>DDPG</center></h3>"
      ],
      "metadata": {
        "id": "3LGSCY_mtcjG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Names:  \n",
        "<font color='red'>\n",
        "Zuriya Ansbacher ID. 208532515  \n",
        "Akiva Bruno Melka ID. 332629393  \n",
        "</font>"
      ],
      "metadata": {
        "id": "xG00NxHutg2C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/zuriyaAnsbacher/Reinforcement_Project"
      ],
      "metadata": {
        "id": "J1KXX7AVtjsu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Install Environment**"
      ],
      "metadata": {
        "id": "Xgd1cHlVtoEi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ti31OH3g6gz"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get update\n",
        "!pip install gym\n",
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install piglet\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This code creates a virtual display to draw game images on. \n",
        "# If you are running locally, just ignore it\n",
        "import os\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
        "    !bash ../xvfb start\n",
        "    %env DISPLAY=:1"
      ],
      "metadata": {
        "id": "e5VYiw4XhA_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Import Libraries**"
      ],
      "metadata": {
        "id": "A1xHTdRPtreo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) # error only\n",
        "from gym.wrappers.monitoring import video_recorder\n",
        "\n",
        "import os\n",
        "import copy\n",
        "import random\n",
        "import argparse\n",
        "import numpy as np\n",
        "from statistics import mean\n",
        "from collections import deque\n",
        "from dataclasses import dataclass\n",
        "from itertools import product\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import tensorflow as tf\n",
        "import torch.optim as optim\n",
        "from torch.optim import Adam\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay"
      ],
      "metadata": {
        "id": "l14BPIxPhDbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "metadata": {
        "id": "dEKretAvhF9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym[box2d]"
      ],
      "metadata": {
        "id": "KCSubaQ-hHog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Utility functions**"
      ],
      "metadata": {
        "id": "UP0pkOoJts10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video(file):\n",
        "  mp4list = glob.glob(file)\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env\n",
        "\n",
        "\"\"\" Utils Functions \"\"\"\n",
        "\n",
        "def set_seed(env, seed):\n",
        "    np.random.seed(seed)\n",
        "    env.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def initialize_layer(layer, w):\n",
        "    layer.weight.data.uniform_(-w, w)\n",
        "    layer.bias.data.uniform_(-w, w)\n",
        "\n",
        "\n",
        "def test(agent, env, paths, model_name, episode_num=100, steps_num=1000, record=False, record_freq=50):\n",
        "  print(\"Start evaluation...\")\n",
        "\n",
        "  agent.set_eval()\n",
        "  scores_list = []\n",
        "  success = False\n",
        "\n",
        "  for episode in range(1, episode_num + 1):\n",
        "      env = wrap_env(env)  #todo: check\n",
        "      state = env.reset()\n",
        "      score = 0\n",
        "\n",
        "      for step in range(1, steps_num + 1):\n",
        "\n",
        "        action = agent.get_action(state, just_greedy=True)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        score += reward\n",
        "        state = next_state\n",
        "        if done: \n",
        "          break\n",
        "\n",
        "      scores_list.append(score)\n",
        "      print(f'Episode: {episode}, Test score: {score:.02f}')\n",
        "\n",
        "  if mean(scores_list) >= 200:\n",
        "    success = True\n",
        "    plt.axhline(200, c='gray', label='Goal', alpha=0.7)\n",
        "    plt.plot(scores_list, 'c', label='Score', alpha=0.7)\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.xlabel('Episodes')\n",
        "    plt.ylabel('Scores')\n",
        "    plt.title(f'{model_name} Test. Average score: {mean(scores_list):.02f}')\n",
        "    plt.show()\n",
        "\n",
        "    plt.close()\n",
        "\n",
        "  else:\n",
        "    print(f'The evaluation has not achieved the goal yet. The model will be training more..')\n",
        "    # env.monitor.close()  #todo: check \n",
        "\n",
        "  return success"
      ],
      "metadata": {
        "id": "CYIJQv3OhtiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@ dataclass\n",
        "class Sample:\n",
        "    state: np.ndarray\n",
        "    action: int or np.ndarray\n",
        "    reward: float\n",
        "    next_state: np.ndarray\n",
        "    done: bool\n",
        "\n",
        "\n",
        "class SamplesMemory:\n",
        "    def __init__(self, max_size, device):\n",
        "        self.max_size = max_size\n",
        "        self.device = device\n",
        "        self.memory_buffer = deque(maxlen=max_size)  # maxlen ensure that samples num won't exceed\n",
        "\n",
        "    def add_sample(self, state, action, reward, next_state, done):\n",
        "        sample = Sample(state, action, reward, next_state, done)\n",
        "        self.memory_buffer.append(sample)\n",
        "\n",
        "    def get_batch(self, batch_size, continuous_action=False):\n",
        "        batch = random.sample(self.memory_buffer, batch_size)\n",
        "        f = lambda x, my_type: torch.tensor(np.vstack(x), device=self.device, dtype=my_type)\n",
        "\n",
        "        state_batch = f([sample.state for sample in batch], torch.float)\n",
        "        action_batch = f([sample.action for sample in batch], torch.float) if continuous_action \\\n",
        "            else f([sample.action for sample in batch], torch.long)\n",
        "        reward_batch = f([sample.reward for sample in batch], torch.float)\n",
        "        next_state_batch = f([sample.next_state for sample in batch], torch.float)\n",
        "        done_batch = f([sample.done for sample in batch], torch.float)\n",
        "\n",
        "        return state_batch, action_batch, reward_batch, next_state_batch, done_batch"
      ],
      "metadata": {
        "id": "F6TZNnebiT7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Ornstein-Uhlenbeck Noise \"\"\"\n",
        "\n",
        "class OU_Noise:\n",
        "    def __init__(self, action_size, mu=0., theta=0.15, sigma=0.2):\n",
        "        self.action_size = action_size\n",
        "        self.mu = np.ones(self.action_size) * mu\n",
        "        self.theta = theta\n",
        "        self.sigma = sigma\n",
        "        self.state = None\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = copy.copy(self.mu)\n",
        "\n",
        "    def add_noise(self):\n",
        "        x = self.state\n",
        "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(len(x))\n",
        "        self.state = x + dx\n",
        "        return self.state"
      ],
      "metadata": {
        "id": "cHfFjvK0iXEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Models**"
      ],
      "metadata": {
        "id": "cCMKzrkft7Cw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Models \"\"\"\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidd1_size, hidd2_size, weight_init=3e-3):\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Sequential(nn.Linear(input_size, hidd1_size), nn.ReLU())\n",
        "        self.fc2 = nn.Sequential(nn.Linear(hidd1_size, hidd2_size), nn.ReLU())\n",
        "        self.fc3 = nn.Linear(hidd2_size, output_size)\n",
        "\n",
        "        self.weight_init = weight_init\n",
        "        initialize_layer(self.fc3, self.weight_init)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = torch.tanh(self.fc3(x))\n",
        "        return x\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, input_size, action_size, hidd1_size, hidd2_size, weight_init=3e-3):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Sequential(nn.Linear(input_size, hidd1_size), nn.ReLU())\n",
        "        self.fc2 = nn.Sequential(nn.Linear(hidd1_size + action_size, hidd2_size), nn.ReLU())\n",
        "        self.fc3 = nn.Linear(hidd2_size, 1)\n",
        "\n",
        "        self.weight_init = weight_init\n",
        "        initialize_layer(self.fc3, self.weight_init)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        x = self.fc1(state)\n",
        "        x = torch.cat((x, action), dim=1)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "VpkUcIQViYqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Agent**"
      ],
      "metadata": {
        "id": "q9vPT060t-Q5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Agent \"\"\"\n",
        "\n",
        "class DDPGAgent:\n",
        "    def __init__(self, state_size, action_size, batch_size, gamma, lr_actor, lr_critic, \n",
        "                 tau, max_eps, min_eps, eps_decay, hidden_actor, hidden_critic, \n",
        "                 memory_size, device):\n",
        "\n",
        "        self.model_name = 'DDPG'\n",
        "\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        self.cur_eps = max_eps\n",
        "        self.min_eps = min_eps\n",
        "        self.eps_decay = eps_decay\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        self.memory = SamplesMemory(memory_size, device)\n",
        "        self.batch_size = batch_size\n",
        "        self.noise = OU_Noise(self.action_size)\n",
        "\n",
        "        self.hidd1_actor = hidden_actor[0]\n",
        "        self.hidd2_actor = hidden_actor[1]\n",
        "        self.actor = Actor(self.state_size, self.action_size, self.hidd1_actor, self.hidd2_actor).to(device)\n",
        "        self.actor_target = Actor(self.state_size, self.action_size, self.hidd1_actor, self.hidd2_actor).to(device)\n",
        "        self.actor_optimizer = Adam(self.actor.parameters(), lr=lr_actor)\n",
        "\n",
        "        self.hidd1_critic = hidden_critic[0]\n",
        "        self.hidd2_critic = hidden_critic[1]\n",
        "        self.critic = Critic(self.state_size, self.action_size, self.hidd1_critic, self.hidd2_critic).to(device)\n",
        "        self.critic_target = Critic(self.state_size, self.action_size, self.hidd1_critic, self.hidd2_critic).to(device)\n",
        "        self.critic_optimizer = Adam(self.critic.parameters(), lr=lr_critic)\n",
        "        self.copy_params_to_target()\n",
        "\n",
        "        self.critic_loss_func = nn.MSELoss()\n",
        "\n",
        "    def get_action(self, state, just_greedy=False):\n",
        "        state = torch.tensor(state, dtype=torch.float, device=self.device)\n",
        "        self.actor.eval()\n",
        "        with torch.no_grad():\n",
        "            action = self.actor(state).cpu().data.numpy()\n",
        "        self.actor.train()\n",
        "        action += self.cur_eps * self.noise.add_noise()\n",
        "        action = np.clip(action, -1, 1)\n",
        "\n",
        "        return action\n",
        "\n",
        "    def decrement_epsilon(self):\n",
        "        return max(self.cur_eps - self.eps_decay, self.min_eps)\n",
        "\n",
        "    def save(self, file_path):\n",
        "        torch.save(self.actor.state_dict(), file_path)\n",
        "\n",
        "    def load(self, file_path):\n",
        "        self.actor.load_state_dict(torch.load(file_path, map_location=self.device))\n",
        "\n",
        "    def set_eval(self):\n",
        "        self.actor.eval()\n",
        "\n",
        "    def copy_params_to_target(self, curr_tau=1.0):\n",
        "        for net, target_net in zip([self.actor, self.critic], [self.actor_target, self.critic_target]):\n",
        "            for params, target_params in zip(net.parameters(), target_net.parameters()):\n",
        "                target_params.data.copy_(params.data * curr_tau + target_params.data * (1.0 - curr_tau))\n",
        "\n",
        "    def learn(self):\n",
        "        states, actions, rewards, next_states, dones = self.memory.get_batch(self.batch_size, continuous_action=True)\n",
        "\n",
        "        # q values\n",
        "        next_action = self.actor_target(next_states)\n",
        "        q_val = self.critic(states, actions)\n",
        "        next_q = self.critic_target(next_states, next_action)\n",
        "\n",
        "        # formula (bellman equation)\n",
        "        q_prime = rewards + (self.gamma * next_q * (1 - dones))\n",
        "\n",
        "        # critic: loss and optimizer\n",
        "        critic_loss = self.critic_loss_func(q_val, q_prime)\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        nn.utils.clip_grad_norm_(self.critic.parameters(), 1)\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "        # actor: loss and optimizer\n",
        "        actions_pred = self.actor(states)\n",
        "        actor_loss = -self.critic(states, actions_pred).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        self.copy_params_to_target(curr_tau=self.tau)\n",
        "\n",
        "        self.cur_eps = self.decrement_epsilon()\n",
        "        self.noise.reset()\n",
        "\n",
        "    def train(self, env, paths, episodes_num, steps_num, learn_freq, update_factor, \n",
        "              record=False, record_freq=50):\n",
        "        scores_list = []\n",
        "        count_test = 1\n",
        "\n",
        "        for episode in range(1, episodes_num + 1):\n",
        "            print(f'Episode: {episode} | Steps: ', end='')\n",
        "            state = env.reset()\n",
        "            self.noise.reset()\n",
        "\n",
        "            score = 0\n",
        "            for step in range(1, steps_num + 1):\n",
        "                if step % 100 == 0:\n",
        "                    print(step, end=' ')\n",
        "                action = self.get_action(state)\n",
        "                next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "                self.memory.add_sample(state, action, reward, next_state, done)\n",
        "                if len(self.memory.memory_buffer) > self.batch_size and step % learn_freq == 0:\n",
        "                    for _ in range(update_factor):\n",
        "                        self.learn()\n",
        "\n",
        "                score += reward\n",
        "                state = next_state\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            print(f'| Score: {score}')\n",
        "            scores_list.append(score)\n",
        "\n",
        "            if len(scores_list) >= 50 and mean(scores_list[-20:]) >= 200 and episode >= count_test + 5:  # last condition: for not doing test every episode, when the learning starts to be better\n",
        "              count_test = episode\n",
        "              # create_plot(scores_list, self.model_name)\n",
        "              success = test(self, env, paths, self.model_name)\n",
        "              if success:\n",
        "                print(f'Mission accomplished in episode {episode}!')\n",
        "                show_video()  # check\n",
        "                break\n",
        "\n",
        "        env.close()"
      ],
      "metadata": {
        "id": "uUCJJ2gdiaA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Main**"
      ],
      "metadata": {
        "id": "Od1B3un3uBaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  parser = argparse.ArgumentParser()\n",
        "  parser.add_argument('--model', default='ddpg')\n",
        "  parser.add_argument('--train', default=True, help='train agent')\n",
        "\n",
        "  parser.add_argument('--memory_size', type=int, default=100000, help='memory buffer size')\n",
        "  parser.add_argument('--episodes', type=int, default=800, help='Number of episodes used in training')\n",
        "  parser.add_argument('--max_eps', default=1.0)\n",
        "  parser.add_argument('--min_eps', default=0.01)\n",
        "  \n",
        "\n",
        "  parser.add_argument('--max_steps', default=800)\n",
        "  parser.add_argument('--batch_size', default=256)\n",
        "  parser.add_argument('--gamma', default=0.9849902858712885)\n",
        "  parser.add_argument('--tau', default=0.008711453313535608)\n",
        "  parser.add_argument('--lr_actor', default=0.0021267736280409758)\n",
        "  parser.add_argument('--lr_critic', default=0.0032480168831600692)\n",
        "  parser.add_argument('--learn_freq', default=8)\n",
        "  parser.add_argument('--update_factor', default=10)\n",
        "  parser.add_argument('--eps_decay', default=0.0008006776266587679)\n",
        "  parser.add_argument('--hidden_actor', default=[256, 128])\n",
        "  parser.add_argument('--hidden_critic', default=[128, 128])\n",
        "\n",
        "  args = parser.parse_args(args=[])\n",
        "\n",
        "  paths = None\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  print(device)\n",
        "\n",
        "  env = gym.make('LunarLanderContinuous-v2')\n",
        "  env.reset()\n",
        "\n",
        "  state_size = env.observation_space.shape[0] \n",
        "  action_size = env.action_space.shape[0]\n",
        "\n",
        "  agent_params = [state_size, action_size, args.batch_size, args.gamma, args.lr_actor,\n",
        "                    args.lr_critic, args.tau, args.max_eps, args.min_eps, args.eps_decay,\n",
        "                    args.hidden_actor, args.hidden_critic, args.memory_size, device]\n",
        "\n",
        "  agent = DDPGAgent(*agent_params)\n",
        "\n",
        "  if args.train:\n",
        "    train_params = [env, paths, args.episodes, args.max_steps, args.learn_freq, args.update_factor]\n",
        "    agent.train(*train_params)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()"
      ],
      "metadata": {
        "id": "c0it5Gujibhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "i2JAHqSiwAzm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}